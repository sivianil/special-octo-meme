{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwfnn2SPPNpSy005k1z4bn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sivianil/special-octo-meme/blob/master/DNA_sequencing_error_correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDct8bFSBkSY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.random import randint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nums = list(range(10))\n",
        "print(nums)\n",
        "print(nums[4:6])\n",
        "print(nums[2:])\n",
        "print(nums[:-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_v51pmQBwYJ",
        "outputId": "20d4d842-e6c6-44e1-b09e-2c0a5a6e2abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "[4, 5]\n",
            "[2, 3, 4, 5, 6, 7, 8, 9]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "players = ['Virat', 'Rohit', 'Surya', 'Sachin', 'Dhoni']\n",
        "for idx, player in enumerate(players):\n",
        "  print('#%d: %s' % (idx+1, player))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpSWKsDaCC15",
        "outputId": "9e1e4da1-b040-44e7-af50-53627a5652d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#1: Virat\n",
            "#2: Rohit\n",
            "#3: Surya\n",
            "#4: Sachin\n",
            "#5: Dhoni\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#List comprehension\n",
        "num = randint(0, 5, size = 4)\n",
        "squares = [x ** 2 for x in num]\n",
        "print(squares)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LebLq-o-CwsQ",
        "outputId": "d702d9d1-b811-443f-fb2e-6e2b2a1c3022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16, 4, 16, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dictionary comprehension\n",
        "nums = list(range(10))\n",
        "square_odd_num = {x: x ** 2 for x in nums if x % 2 != 0}\n",
        "print(square_odd_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4Zm76LgDWG_",
        "outputId": "d512090a-bc9f-43b1-a4fe-0dfcbd1bfb11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 1, 3: 9, 5: 25, 7: 49, 9: 81}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set comprehension\n",
        "from math import sqrt\n",
        "nums = {int(sqrt(x)) for x in range(50)}\n",
        "print(nums)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LkaPZpEEOY1",
        "outputId": "6bae09a0-8c91-4b04-9109-598ac4cd174c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0, 1, 2, 3, 4, 5, 6, 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tuple immutable ordered list of values. Can be used as keys in dictionaries and elements of sets while lists cannot\n",
        "d = {(x, x+1): x ** 2 for x in range(15)}\n",
        "print(d.keys())\n",
        "print(d.values())\n",
        "print(d[7,8])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIaNxzcIE3ji",
        "outputId": "e6690f35-d6f0-4bc2-a71f-88f29602a5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys([(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15)])\n",
            "dict_values([0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196])\n",
            "49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic functions\n",
        "#Write a function to return the sign of a given number \n",
        "def sign(x):\n",
        "\n",
        "  if x > 0:\n",
        "    return 'Positive'\n",
        "  elif x < 0:\n",
        "    return 'Negative'\n",
        "  else:\n",
        "    return 'zero'\n",
        "\n",
        "print(sign(-(-(-5))))\n",
        "print(sign(-(-5)))\n",
        "\n",
        "for x in [-3, 0, 5]:\n",
        "  print(sign(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OVKWdiCFkVr",
        "outputId": "80558e47-4e64-4bd2-c972-1911307a4c90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative\n",
            "Positive\n",
            "Negative\n",
            "zero\n",
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Arrays**\n",
        "\n"
      ],
      "metadata": {
        "id": "tRQcc3fZFd6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ar = np.array([2, 5, 7, 9, 11])\n",
        "print(type(ar))\n",
        "print(ar[4])\n",
        "ar[3] = 15\n",
        "print(ar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOOgOflcGwbg",
        "outputId": "c9689c45-90ec-4b87-8ce0-f6d00b3e8206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "11\n",
            "[ 2  5  7 15 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install google-nucleus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nZc-BGaFywJ",
        "outputId": "8b62ff57-d9ee-4876-ce87-08be2ebde4f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting google-nucleus\n",
            "  Downloading google_nucleus-0.6.0.tar.gz (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.8/dist-packages (from google-nucleus) (0.5.5)\n",
            "Requirement already satisfied: intervaltree in /usr/local/lib/python3.8/dist-packages (from google-nucleus) (2.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from google-nucleus) (1.4.0)\n",
            "Collecting mock\n",
            "  Downloading mock-5.0.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from google-nucleus) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from google-nucleus) (1.15.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from google-nucleus) (3.19.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from google-nucleus) (7.1.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from google-nucleus) (7.9.0)\n",
            "Collecting apache-beam\n",
            "  Downloading apache_beam-2.44.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.8/dist-packages (from apache-beam->google-nucleus) (1.7)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->google-nucleus) (2.8.2)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam->google-nucleus) (1.22.2)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam->google-nucleus) (1.51.1)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.8/dist-packages (from apache-beam->google-nucleus) (2022.7.1)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.8/dist-packages (from apache-beam->google-nucleus) (2022.6.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->google-nucleus) (2.25.1)\n",
            "Collecting fastavro<2,>=0.23.6\n",
            "  Downloading fastavro-1.7.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting objsize<0.7.0,>=0.6.1\n",
            "  Downloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n",
            "Collecting zstandard<1,>=0.18.0\n",
            "  Downloading zstandard-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httplib2<0.21.0,>=0.8 in /usr/local/lib/python3.8/dist-packages (from apache-beam->google-nucleus) (0.17.4)\n",
            "Requirement already satisfied: cloudpickle~=2.2.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->google-nucleus) (2.2.1)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.8.5-cp38-cp38-manylinux_2_28_x86_64.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->google-nucleus) (4.4.0)\n",
            "Requirement already satisfied: pyarrow<10.0.0,>=0.15.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam->google-nucleus) (9.0.0)\n",
            "Collecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.2/526.2 KB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 KB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->google-nucleus) (1.3.0)\n",
            "Collecting fasteners<1.0,>=0.3\n",
            "  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.8/dist-packages (from intervaltree->google-nucleus) (2.4.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython->google-nucleus) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->google-nucleus) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->google-nucleus) (2.0.10)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->google-nucleus) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->google-nucleus) (4.8.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->google-nucleus) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->google-nucleus) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->google-nucleus) (5.7.1)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->google-nucleus) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->google-nucleus) (0.2.6)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.8/dist-packages (from pydot<2,>=1.2.0->apache-beam->google-nucleus) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->google-nucleus) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->google-nucleus) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->google-nucleus) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->google-nucleus) (1.24.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython->google-nucleus) (0.7.0)\n",
            "Building wheels for collected packages: google-nucleus, dill, docopt\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for google-nucleus (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for google-nucleus\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for google-nucleus\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=8f291946a7f3a7720b760255062ed9e02f8ed80b4439b88d606859209f802335\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/35/78/e9004fa30578734db7f10e7a211605f3f0778d2bdde38a239d\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=c5e9f7cb6bbcdfba79a356f7372c35f632b251463987228fb36503d095499d23\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "Successfully built dill docopt\n",
            "Failed to build google-nucleus\n",
            "Installing collected packages: docopt, zstandard, pymongo, orjson, objsize, mock, jedi, fasteners, fastavro, dill, hdfs, apache-beam, google-nucleus\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.3.3\n",
            "    Uninstalling pymongo-4.3.3:\n",
            "      Successfully uninstalled pymongo-4.3.3\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "  Running setup.py install for google-nucleus ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: google-nucleus was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed apache-beam-2.44.0 dill-0.3.1.1 docopt-0.6.2 fastavro-1.7.1 fasteners-0.18 google-nucleus-0.6.0 hdfs-2.7.0 jedi-0.18.2 mock-5.0.1 objsize-0.6.1 orjson-3.8.5 pymongo-3.13.0 zstandard-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pysam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL3DccSIoV4Q",
        "outputId": "cb7e1dcf-3351-4a51-b84e-e896f2483fca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pysam\n",
            "  Downloading pysam-0.20.0-cp38-cp38-manylinux_2_24_x86_64.whl (16.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pysam\n",
            "Successfully installed pysam-0.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pysam\n",
        "\n",
        "from nucleus.io import fasta\n",
        "from nucleus.io import sam\n",
        "from nucleus.io import vcf\n",
        "from nucleus.io.genomics_writer import TFRecordWriter\n",
        "from nucleus.protos import reads_pb2\n",
        "from nucleus.util import cigar\n",
        "from nucleus.util import ranges\n",
        "from nucleus.util import utils\n",
        "\n",
        "# Import TensorFlow after Nucleus.\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "t967Ihq3MVKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz -O hs37d5.fa.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_STrvn1xt1Qv",
        "outputId": "124f77c5-a1a3-4f8b-bd96-0449a5cc271e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-07 16:31:17--  ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz\n",
            "           => ‘hs37d5.fa.gz’\n",
            "Resolving ftp.1000genomes.ebi.ac.uk (ftp.1000genomes.ebi.ac.uk)... 193.62.193.140\n",
            "Connecting to ftp.1000genomes.ebi.ac.uk (ftp.1000genomes.ebi.ac.uk)|193.62.193.140|:21... connected.\n",
            "Logging in as anonymous ... Logged in!\n",
            "==> SYST ... done.    ==> PWD ... done.\n",
            "==> TYPE I ... done.  ==> CWD (1) /vol1/ftp/technical/reference/phase2_reference_assembly_sequence ... done.\n",
            "==> SIZE hs37d5.fa.gz ... 892326179\n",
            "==> PASV ... done.    ==> RETR hs37d5.fa.gz ... done.\n",
            "Length: 892326179 (851M) (unauthoritative)\n",
            "\n",
            "hs37d5.fa.gz        100%[===================>] 850.99M  12.0MB/s    in 3m 53s  \n",
            "\n",
            "2023-02-07 16:35:14 (3.65 MB/s) - ‘hs37d5.fa.gz’ saved [892326179]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz.fai -O hs37d5.fa.gz.fai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZX772OmvLNo",
        "outputId": "24cac28a-3d4c-4fdc-eb9f-8f0483e05637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-07 16:37:12--  ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz.fai\n",
            "           => ‘hs37d5.fa.gz.fai’\n",
            "Resolving ftp.1000genomes.ebi.ac.uk (ftp.1000genomes.ebi.ac.uk)... 193.62.193.140\n",
            "Connecting to ftp.1000genomes.ebi.ac.uk (ftp.1000genomes.ebi.ac.uk)|193.62.193.140|:21... connected.\n",
            "Logging in as anonymous ... Logged in!\n",
            "==> SYST ... done.    ==> PWD ... done.\n",
            "==> TYPE I ... done.  ==> CWD (1) /vol1/ftp/technical/reference/phase2_reference_assembly_sequence ... done.\n",
            "==> SIZE hs37d5.fa.gz.fai ... 2813\n",
            "==> PASV ... done.    ==> RETR hs37d5.fa.gz.fai ... done.\n",
            "Length: 2813 (2.7K) (unauthoritative)\n",
            "\n",
            "hs37d5.fa.gz.fai    100%[===================>]   2.75K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-02-07 16:37:15 (49.8 KB/s) - ‘hs37d5.fa.gz.fai’ saved [2813]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz.gzi -O hs37d5.fa.gz.gzi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pR70d6MOvS54",
        "outputId": "f98ef2a5-829a-4cd9-d152-9fbc49d3fca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-07 16:37:41--  ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz.gzi\n",
            "           => ‘hs37d5.fa.gz.gzi’\n",
            "Resolving ftp.1000genomes.ebi.ac.uk (ftp.1000genomes.ebi.ac.uk)... 193.62.193.140\n",
            "Connecting to ftp.1000genomes.ebi.ac.uk (ftp.1000genomes.ebi.ac.uk)|193.62.193.140|:21... connected.\n",
            "Logging in as anonymous ... Logged in!\n",
            "==> SYST ... done.    ==> PWD ... done.\n",
            "==> TYPE I ... done.  ==> CWD (1) /vol1/ftp/technical/reference/phase2_reference_assembly_sequence ... done.\n",
            "==> SIZE hs37d5.fa.gz.gzi ... 778760\n",
            "==> PASV ... done.    ==> RETR hs37d5.fa.gz.gzi ... done.\n",
            "Length: 778760 (761K) (unauthoritative)\n",
            "\n",
            "hs37d5.fa.gz.gzi    100%[===================>] 760.51K   541KB/s    in 1.4s    \n",
            "\n",
            "2023-02-07 16:37:46 (541 KB/s) - ‘hs37d5.fa.gz.gzi’ saved [778760]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \\\n",
        "ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv3.3.2/GRCh37/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz \\\n",
        "-O NA12878_calls.vcf.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFuW5S6cvc-i",
        "outputId": "5e01e396-f996-44c2-cc95-595a279d79fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-07 16:38:07--  ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv3.3.2/GRCh37/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz\n",
            "           => ‘NA12878_calls.vcf.gz’\n",
            "Resolving ftp-trace.ncbi.nlm.nih.gov (ftp-trace.ncbi.nlm.nih.gov)... 130.14.250.10, 130.14.250.11, 2607:f220:41f:250::228, ...\n",
            "Connecting to ftp-trace.ncbi.nlm.nih.gov (ftp-trace.ncbi.nlm.nih.gov)|130.14.250.10|:21... connected.\n",
            "Logging in as anonymous ... Logged in!\n",
            "==> SYST ... done.    ==> PWD ... done.\n",
            "==> TYPE I ... done.  ==> CWD (1) /giab/ftp/release/NA12878_HG001/NISTv3.3.2/GRCh37 ... done.\n",
            "==> SIZE HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz ... 134602007\n",
            "==> PASV ... done.    ==> RETR HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz ... done.\n",
            "Length: 134602007 (128M) (unauthoritative)\n",
            "\n",
            "HG001_GRCh37_GIAB_h 100%[===================>] 128.37M  16.6MB/s    in 9.1s    \n",
            "\n",
            "2023-02-07 16:38:19 (14.1 MB/s) - ‘NA12878_calls.vcf.gz’ saved [134602007]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \\\n",
        "ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv3.3.2/GRCh37/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz.tbi \\\n",
        "-O NA12878_calls.vcf.gz.tbi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e5Ya5DLvmeX",
        "outputId": "ba6fd3e9-4efa-4e4e-a5e8-ce0cbffeea92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-07 16:38:51--  ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv3.3.2/GRCh37/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz.tbi\n",
            "           => ‘NA12878_calls.vcf.gz.tbi’\n",
            "Resolving ftp-trace.ncbi.nlm.nih.gov (ftp-trace.ncbi.nlm.nih.gov)... 165.112.9.228, 165.112.9.230, 2607:f220:41e:250::13, ...\n",
            "Connecting to ftp-trace.ncbi.nlm.nih.gov (ftp-trace.ncbi.nlm.nih.gov)|165.112.9.228|:21... connected.\n",
            "Logging in as anonymous ... Logged in!\n",
            "==> SYST ... done.    ==> PWD ... done.\n",
            "==> TYPE I ... done.  ==> CWD (1) /giab/ftp/release/NA12878_HG001/NISTv3.3.2/GRCh37 ... done.\n",
            "==> SIZE HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz.tbi ... 1607059\n",
            "==> PASV ... done.    ==> RETR HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz.tbi ... done.\n",
            "Length: 1607059 (1.5M) (unauthoritative)\n",
            "\n",
            "HG001_GRCh37_GIAB_h 100%[===================>]   1.53M  1.33MB/s    in 1.2s    \n",
            "\n",
            "2023-02-07 16:38:55 (1.33 MB/s) - ‘NA12878_calls.vcf.gz.tbi’ saved [1607059]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/NIST_NA12878_HG001_HiSeq_300x/RMNISTHS_30xdownsample.bam -O NA12878sample.bam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROS5RlApwHVU",
        "outputId": "5a0f3978-6dd4-4fb7-d94b-8fa0c7c44303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-07 16:42:06--  ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/NIST_NA12878_HG001_HiSeq_300x/RMNISTHS_30xdownsample.bam\n",
            "           => ‘NA12878sample.bam’\n",
            "Resolving ftp-trace.ncbi.nlm.nih.gov (ftp-trace.ncbi.nlm.nih.gov)... 130.14.250.13, 165.112.9.228, 2607:f220:41e:250::7, ...\n",
            "Connecting to ftp-trace.ncbi.nlm.nih.gov (ftp-trace.ncbi.nlm.nih.gov)|130.14.250.13|:21... connected.\n",
            "Logging in as anonymous ... Logged in!\n",
            "==> SYST ... done.    ==> PWD ... done.\n",
            "==> TYPE I ... done.  ==> CWD (1) /giab/ftp/data/NA12878/NIST_NA12878_HG001_HiSeq_300x ... done.\n",
            "==> SIZE RMNISTHS_30xdownsample.bam ... 158166081112\n",
            "==> PASV ... done.    ==> RETR RMNISTHS_30xdownsample.bam ... done.\n",
            "Length: 158166081112 (147G) (unauthoritative)\n",
            "\n",
            "STHS_30xdownsample.   0%[                    ]  62.03M  16.6MB/s    eta 3h 32m ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# The line above prevents this cell from running as the\n",
        "# default Colab environment does not include the necessary software.\n",
        "\n",
        "# NA12878_sliced.bam\n",
        "samtools view -h \\\n",
        "ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/NIST_NA12878_HG001_HiSeq_300x/RMNISTHS_30xdownsample.bam \\\n",
        "20:10,000,000-10,100,000 \\\n",
        "-o NA12878_sliced.bam\n",
        "\n",
        "\n",
        "# NA12878_sliced.bam.bai`\n",
        "samtools index NA12878_sliced.bam\n",
        "\n",
        "# NA12878_calls.vcf.gz\n",
        "!wget \\\n",
        "ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv3.3.2/GRCh37/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz \\\n",
        "-O NA12878_calls.vcf.gz\n",
        "\n",
        "\n",
        "# NA12878_calls.vcf.gz.tbi\n",
        "!wget \\\n",
        "ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv3.3.2/GRCh37/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz.tbi \\\n",
        "-O NA12878_calls.vcf.gz.tbi\n",
        "\n",
        "\n",
        "# hs37d5.fa.gz\n",
        "!wget \\\n",
        "ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz \\\n",
        "-O hs37d5.fa.gz\n",
        "\n",
        "\n",
        "# hs37d5.fa.gz.fai\n",
        "!wget \\\n",
        "ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz.fai \\\n",
        "-O hs37d5.fa.gz.fai\n",
        "\n",
        "\n",
        "# hs37d5.fa.gz.gzi\n",
        "!wget \\\n",
        "ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz.gzi \\\n",
        "-O hs37d5.fa.gz.gzi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "PqQ0h3Jvnmrs",
        "outputId": "7ad29f42-01e7-4420-c676-0b2d23a16755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-42-e4776dbdb61e>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv3.3.2/GRCh37/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz.tbi \\\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants and utility functions.\n",
        "\n",
        "# We will only allow simple alignments, specified by the below cigar string\n",
        "# operators. If you are not familiar with cigar strings, you can read more\n",
        "# at section 1.4 of this link: https://samtools.github.io/hts-specs/SAMv1.pdf\n",
        "_ALLOWED_CIGAR_OPS = frozenset([cigar.CHAR_TO_CIGAR_OPS[op] for op in 'MX='])\n",
        "\n",
        "# We will only allow certain bases.\n",
        "_ALLOWED_BASES = 'ACGT'\n",
        "\n",
        "_TRAIN = 'train.tfrecord'\n",
        "_EVAL = 'eval.tfrecord'\n",
        "_TEST = 'test.tfrecord'"
      ],
      "metadata": {
        "id": "u1CY16s4Mr2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hparams):\n",
        "  \"\"\"Convolutional neural network architecture.\"\"\"\n",
        "\n",
        "  l2_reg = tf.keras.regularizers.l2\n",
        "\n",
        "  return tf.keras.models.Sequential([\n",
        "\n",
        "      # Two convolution + maxpooling blocks\n",
        "      layers.Conv1D(\n",
        "          filters=16,\n",
        "          kernel_size=5,\n",
        "          activation=tf.nn.relu,\n",
        "          kernel_regularizer=l2_reg(hparams.l2)),\n",
        "      layers.MaxPool1D(pool_size=3, strides=1),\n",
        "      layers.Conv1D(\n",
        "          filters=16,\n",
        "          kernel_size=3,\n",
        "          activation=tf.nn.relu,\n",
        "          kernel_regularizer=l2_reg(hparams.l2)),\n",
        "      layers.MaxPool1D(pool_size=3, strides=1),\n",
        "\n",
        "      # Flatten the input volume\n",
        "      layers.Flatten(),\n",
        "\n",
        "      # Two fully connected layers, each followed by a dropout layer\n",
        "      layers.Dense(\n",
        "          units=16,\n",
        "          activation=tf.nn.relu,\n",
        "          kernel_regularizer=l2_reg(hparams.l2)),\n",
        "      layers.Dropout(rate=0.3),\n",
        "      layers.Dense(\n",
        "          units=16,\n",
        "          activation=tf.nn.relu,\n",
        "          kernel_regularizer=l2_reg(hparams.l2)),\n",
        "      layers.Dropout(rate=0.3),\n",
        "\n",
        "      # Output layer with softmax activation\n",
        "      layers.Dense(units=len(_ALLOWED_BASES), activation='softmax')\n",
        "  ])"
      ],
      "metadata": {
        "id": "fW1jU1nGNXPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement Neural Network pipeline\n",
        "\n",
        "The pipeline consists of three steps:\n",
        "\n",
        "\n",
        "1.   **Generate TFRecords Datasets**\n",
        "2.   **Read data from TFRecords datasets**\n",
        "3.   **Train the model**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M-rzU19ci4Fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_tfrecord_datasets(hparams):\n",
        "  \"\"\"Writes out TFRecords files for training, evaluation, and test datasets.\"\"\"\n",
        "  if not os.path.exists(hparams.out_dir):\n",
        "    os.makedirs(hparams.out_dir)\n",
        "\n",
        "  # Fraction of examples in each dataset.\n",
        "  train_eval_test_split = [0.7, 0.2, 0.1]\n",
        "  num_train_examples = 0\n",
        "  num_eval_examples = 0\n",
        "  num_test_examples = 0\n",
        "\n",
        "  # Generate training, test, and evaluation examples.\n",
        "  with TFRecordWriter(os.path.join(hparams.out_dir, _TRAIN)) as train_out, \\\n",
        "       TFRecordWriter(os.path.join(hparams.out_dir, _EVAL)) as eval_out, \\\n",
        "       TFRecordWriter(os.path.join(hparams.out_dir, _TEST)) as test_out:\n",
        "    all_examples = make_ngs_examples(hparams)\n",
        "    for example in all_examples:\n",
        "      r = random.random()\n",
        "      if r < train_eval_test_split[0]:\n",
        "        train_out.write(proto=example)\n",
        "        num_train_examples += 1\n",
        "      elif r < train_eval_test_split[0] + train_eval_test_split[1]:\n",
        "        eval_out.write(proto=example)\n",
        "        num_eval_examples += 1\n",
        "      else:\n",
        "        test_out.write(proto=example)\n",
        "        num_test_examples += 1\n",
        "  print('# of training examples: %d' % num_train_examples)\n",
        "  print('# of evaluation examples: %d' % num_eval_examples)\n",
        "  print('# of test examples: %d' % num_test_examples)\n",
        "\n",
        "\n",
        "def make_ngs_examples(hparams):\n",
        "  \"\"\"Generator function that yields training, evaluation and test examples.\"\"\"\n",
        "  ref_reader = fasta.IndexedFastaReader(input_path=hparams.ref_path)\n",
        "  vcf_reader = vcf.VcfReader(input_path=hparams.vcf_path)\n",
        "  read_requirements = reads_pb2.ReadRequirements()\n",
        "  sam_reader = sam.SamReader(\n",
        "      input_path=hparams.bam_path, read_requirements=read_requirements)\n",
        "\n",
        "  # Use a separate SAM reader to query for reads falling in the pileup range.\n",
        "  sam_query_reader = sam.SamReader(\n",
        "      input_path=hparams.bam_path, read_requirements=read_requirements)\n",
        "  used_pileup_ranges = set()\n",
        "  with ref_reader, vcf_reader, sam_reader, sam_query_reader:\n",
        "    for read in sam_reader:\n",
        "\n",
        "      # Check that read has cigar string present and allowed alignment.\n",
        "      if not read.alignment.cigar:\n",
        "        print('Skipping read, no cigar alignment found')\n",
        "        continue\n",
        "      if not has_allowed_alignment(read):\n",
        "        continue\n",
        "\n",
        "      # Obtain window that will be used to construct an example.\n",
        "      read_range = utils.read_range(read)\n",
        "      ref = ref_reader.query(region=read_range)\n",
        "      pileup_range = get_pileup_range(hparams, read, read_range, ref)\n",
        "\n",
        "      # Do not construct multiple examples with the same pileup range.\n",
        "      pileup_range_serialized = pileup_range.SerializeToString()\n",
        "      if pileup_range_serialized in used_pileup_ranges:\n",
        "        continue\n",
        "      used_pileup_ranges.add(pileup_range_serialized)\n",
        "\n",
        "      # Get reference sequence, reads, and truth variants for the pileup range.\n",
        "      pileup_reads = list(sam_query_reader.query(region=pileup_range))\n",
        "      pileup_ref = ref_reader.query(region=pileup_range)\n",
        "      pileup_variants = list(vcf_reader.query(region=pileup_range))\n",
        "      if is_usable_example(pileup_reads, pileup_variants, pileup_ref):\n",
        "        yield make_example(hparams, pileup_reads, pileup_ref, pileup_range)\n",
        "\n",
        "\n",
        "def get_pileup_range(hparams, read, read_range, ref):\n",
        "  \"\"\"Returns a range that will be used to construct one example.\"\"\"\n",
        "\n",
        "  # Find error positions where read and reference differ.\n",
        "  ngs_read_length = read_range.end - read_range.start\n",
        "  error_indices = [\n",
        "      i for i in range(ngs_read_length) if ref[i] != read.aligned_sequence[i]\n",
        "  ]\n",
        "\n",
        "  # If read and reference sequence are the same, create an example centered\n",
        "  # at middle base of read.\n",
        "  if not error_indices:\n",
        "    error_idx = ngs_read_length // 2\n",
        "\n",
        "  # If read and reference differ at one or more positions, create example\n",
        "  # centered at a random error position.\n",
        "  else:\n",
        "    error_idx = random.choice(error_indices)\n",
        "\n",
        "  error_pos = read_range.start + error_idx\n",
        "  flank_size = hparams.window_size // 2\n",
        "  return ranges.make_range(\n",
        "      chrom=read_range.reference_name,\n",
        "      start=error_pos - flank_size,\n",
        "      end=error_pos + flank_size + 1)\n",
        "\n",
        "\n",
        "def has_allowed_alignment(read):\n",
        "  \"\"\"Determines whether a read's CIGAR string has the allowed alignments.\"\"\"\n",
        "  return all([c.operation in _ALLOWED_CIGAR_OPS for c in read.alignment.cigar])\n",
        "\n",
        "\n",
        "def is_usable_example(reads, variants, ref_bases):\n",
        "  \"\"\"Determines whether a particular reference region and read can be used.\"\"\"\n",
        "  # Discard examples with variants or no mapped reads.\n",
        "  if variants or not reads:\n",
        "    return False\n",
        "\n",
        "  # Use only examples where all reads have simple alignment and allowed bases.\n",
        "  for read in reads:\n",
        "    if not has_allowed_alignment(read):\n",
        "      return False\n",
        "    if any(base not in _ALLOWED_BASES for base in read.aligned_sequence):\n",
        "      return False\n",
        "\n",
        "  # Reference should only contain allowed bases.\n",
        "  if any(base not in _ALLOWED_BASES for base in ref_bases):\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "\n",
        "def make_example(hparams, pileup_reads, pileup_ref, pileup_range):\n",
        "  \"\"\"Takes in an input sequence and outputs tf.train.Example ProtocolMessages.\n",
        "\n",
        "  Each example contains the following features: A counts, C counts, G counts,\n",
        "  T counts, reference sequence, correct base label.\n",
        "  \"\"\"\n",
        "  assert len(pileup_ref) == hparams.window_size\n",
        "  example = tf.train.Example()\n",
        "  base_counts = np.zeros(shape=[hparams.window_size, len(_ALLOWED_BASES)])\n",
        "\n",
        "  for read in pileup_reads:\n",
        "    read_position = read.alignment.position.position\n",
        "    read_ints = [_ALLOWED_BASES.index(b) for b in read.aligned_sequence]\n",
        "    one_hot_read = np.zeros((len(read_ints), len(_ALLOWED_BASES)))\n",
        "    one_hot_read[np.arange(len(one_hot_read)), read_ints] = 1\n",
        "\n",
        "    window_start = read_position - pileup_range.start\n",
        "    window_end = window_start + len(read_ints)\n",
        "\n",
        "    # If read falls outside of window, adjust start/end indices for window.\n",
        "    window_start = max(0, window_start)\n",
        "    window_end = min(window_end, hparams.window_size)\n",
        "\n",
        "    # We consider four possible scenarios for each read and adjust start/end\n",
        "    # indices to only include portions of read that overlap the window.\n",
        "    # 1) Read extends past 5' end of window\n",
        "    # 2) Read extends past 3' end of window\n",
        "    # 3) Read extends past 5' and 3' ends of window\n",
        "    # 4) Read falls entirely within window\n",
        "    if window_start == 0 and window_end != hparams.window_size:\n",
        "      read_start = pileup_range.start - read_position\n",
        "      read_end = None\n",
        "    if window_end == hparams.window_size and window_start != 0:\n",
        "      read_start = None\n",
        "      read_end = -1 * ((read_position + len(read_ints)) - pileup_range.end)\n",
        "    if window_start == 0 and window_end == hparams.window_size:\n",
        "      read_start = pileup_range.start - read_position\n",
        "      read_end = read_start + hparams.window_size\n",
        "    if window_start != 0 and window_end != hparams.window_size:\n",
        "      read_start = None\n",
        "      read_end = None\n",
        "    base_counts[window_start:window_end] += one_hot_read[read_start:read_end]\n",
        "\n",
        "  # Use fractions at each position instead of raw base counts.\n",
        "  base_counts /= np.expand_dims(np.sum(base_counts, axis=-1), -1)\n",
        "\n",
        "  # Save counts/fractions for each base separately.\n",
        "  features = example.features\n",
        "  for i in range(len(_ALLOWED_BASES)):\n",
        "    key = '%s_counts' % _ALLOWED_BASES[i]\n",
        "    features.feature[key].float_list.value.extend(list(base_counts[:, i]))\n",
        "\n",
        "  features.feature['ref_sequence'].int64_list.value.extend(\n",
        "      [_ALLOWED_BASES.index(base) for base in pileup_ref])\n",
        "  flank_size = hparams.window_size // 2\n",
        "  true_base = pileup_ref[flank_size]\n",
        "  features.feature['label'].int64_list.value.append(\n",
        "      _ALLOWED_BASES.index(true_base))\n",
        "\n",
        "  return example"
      ],
      "metadata": {
        "id": "nYevxYGvRp-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(hparams, filename, num_epochs):\n",
        "  \"\"\"Reads in and processes the TFRecords dataset.\n",
        "\n",
        "  Builds a pipeline that returns pairs of features, label.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define field names, types, and sizes for TFRecords.\n",
        "  proto_features = {\n",
        "      'A_counts':\n",
        "          tf.io.FixedLenFeature(shape=[hparams.window_size], dtype=tf.float32),\n",
        "      'C_counts':\n",
        "          tf.io.FixedLenFeature(shape=[hparams.window_size], dtype=tf.float32),\n",
        "      'G_counts':\n",
        "          tf.io.FixedLenFeature(shape=[hparams.window_size], dtype=tf.float32),\n",
        "      'T_counts':\n",
        "          tf.io.FixedLenFeature(shape=[hparams.window_size], dtype=tf.float32),\n",
        "      'ref_sequence':\n",
        "          tf.io.FixedLenFeature(shape=[hparams.window_size], dtype=tf.int64),\n",
        "      'label':\n",
        "          tf.io.FixedLenFeature(shape=[1], dtype=tf.int64),\n",
        "  }\n",
        "\n",
        "  def _process_input(proto_string):\n",
        "    \"\"\"Helper function for input function that parses a serialized example.\"\"\"\n",
        "\n",
        "    parsed_features = tf.io.parse_single_example(\n",
        "        serialized=proto_string, features=proto_features)\n",
        "\n",
        "    # Stack counts/fractions for all bases to create input of dimensions\n",
        "    # `hparams.window_size` x len(_ALLOWED_BASES).\n",
        "    feature_columns = []\n",
        "    for base in _ALLOWED_BASES:\n",
        "      feature_columns.append(parsed_features['%s_counts' % base])\n",
        "    features = tf.stack(feature_columns, axis=-1)\n",
        "    label = parsed_features['label']\n",
        "    return features, label\n",
        "\n",
        "  ds = tf.data.TFRecordDataset(filenames=filename)\n",
        "  ds = ds.map(map_func=_process_input)\n",
        "  ds = ds.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
        "  ds = ds.batch(batch_size=hparams.batch_size).repeat(count=num_epochs)\n",
        "  return ds"
      ],
      "metadata": {
        "id": "CqCucvlCj6lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(hparams, use_existing_data=False, seed=1):\n",
        "  \"\"\"Creates a model, runs training and evaluation.\"\"\"\n",
        "\n",
        "  # Set seed for reproducibility.\n",
        "  random.seed(seed)\n",
        "  tf.random.set_seed(seed)\n",
        "\n",
        "  if not use_existing_data:\n",
        "    print('Generating data...')\n",
        "    generate_tfrecord_datasets(hparams)\n",
        "\n",
        "  train_dataset = get_dataset(\n",
        "      hparams, filename=os.path.join(hparams.out_dir, _TRAIN), num_epochs=1)\n",
        "  eval_dataset = get_dataset(\n",
        "      hparams, filename=os.path.join(hparams.out_dir, _EVAL), num_epochs=1)\n",
        "  test_dataset = get_dataset(\n",
        "      hparams, filename=os.path.join(hparams.out_dir, _TEST), num_epochs=1)\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(lr=hparams.learning_rate)\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      hparams.log_dir, histogram_freq=1, profile_batch=0)\n",
        "  model = build_model(hparams)\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "      metrics=['accuracy'])\n",
        "\n",
        "  print('Training the model. This should take ~6 minutes...')\n",
        "  model.fit(\n",
        "      train_dataset,\n",
        "      epochs=hparams.total_epochs,\n",
        "      validation_data=eval_dataset,\n",
        "      callbacks=[tensorboard_callback],\n",
        "      verbose=0)\n",
        "  print('Training complete. Obtaining final metrics...')\n",
        "  eval_metrics = model.evaluate(eval_dataset, verbose=0)\n",
        "  test_metrics = model.evaluate(test_dataset, verbose=0)\n",
        "  print('Final eval metrics - loss: %f - accuracy: %f' %\n",
        "        (eval_metrics[0], eval_metrics[1]))\n",
        "  print('Final test metrics - loss: %f - accuracy: %f' %\n",
        "        (test_metrics[0], test_metrics[1]))"
      ],
      "metadata": {
        "id": "_PMjQZ9ZnU2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to experiment with different values.\n",
        "# A description of all hyperparameters is provided\n",
        "# in the appendix.\n",
        "\n",
        "class BaseHparams(object):\n",
        "  \"\"\"Default hyperparameters.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               total_epochs=100,\n",
        "               learning_rate=0.004,\n",
        "               l2=0.001,\n",
        "               batch_size=256,\n",
        "               window_size=21,\n",
        "               ref_path='hs37d5.fa.gz',\n",
        "               vcf_path='NA12878_calls.vcf.gz',\n",
        "               bam_path='NA12878_sliced.bam',\n",
        "               out_dir='examples',\n",
        "               model_dir='ngs_model',\n",
        "               log_dir='logs'):\n",
        "\n",
        "    self.total_epochs = total_epochs\n",
        "    self.learning_rate = learning_rate\n",
        "    self.l2 = l2\n",
        "    self.batch_size = batch_size\n",
        "    self.window_size = window_size\n",
        "    self.ref_path = ref_path\n",
        "    self.vcf_path = vcf_path\n",
        "    self.bam_path = bam_path\n",
        "    self.out_dir = out_dir\n",
        "    self.model_dir = model_dir\n",
        "    self.log_dir = log_dir"
      ],
      "metadata": {
        "id": "Xqy1HV4mnZ0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell should take ~6 minutes to run with the default parameters.\n",
        "hparams = BaseHparams()\n",
        "run(hparams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "Ue1e7k3vndND",
        "outputId": "c46316aa-91d6-4578-d01c-3669ac5e8796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-d46fc6eb10e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This cell should take ~6 minutes to run with the default parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseHparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-77c5366ea02f>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(hparams, use_existing_data, seed)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_existing_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Generating data...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mgenerate_tfrecord_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   train_dataset = get_dataset(\n",
            "\u001b[0;32m<ipython-input-30-25c33ad9027c>\u001b[0m in \u001b[0;36mgenerate_tfrecord_datasets\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m     15\u001b[0m        \u001b[0mTFRecordWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_TEST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtest_out\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mall_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_ngs_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_examples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtrain_eval_test_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-25c33ad9027c>\u001b[0m in \u001b[0;36mmake_ngs_examples\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_ngs_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;34m\"\"\"Generator function that yields training, evaluation and test examples.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0mref_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexedFastaReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   \u001b[0mvcf_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVcfReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvcf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mread_requirements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreads_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReadRequirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nucleus/io/fasta.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_path, keep_true_case, cache_size)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcache_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0;31m# Use the C++-defined default cache size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m       self._reader = reference.IndexedFastaReader.from_file(\n\u001b[0m\u001b[1;32m     91\u001b[0m           fasta_path, fai_path, options)\n\u001b[1;32m     92\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Not found: could not load fasta and/or fai for fasta hs37d5.fa.gz"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KuYiABj9ngAV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}